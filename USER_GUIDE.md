# Руководство пользователя: Atlassian Marketplace Scraper

## Содержание

1. [Введение](#введение)
2. [Установка](#установка)
3. [Настройка](#настройка)
4. [Использование](#использование)
5. [Веб-интерфейс](#веб-интерфейс)
6. [Решение проблем](#решение-проблем)
7. [Часто задаваемые вопросы](#часто-задаваемые-вопросы)

## Введение

Atlassian Marketplace Scraper — это инструмент для сбора данных о приложениях из Atlassian Marketplace. Он позволяет:

- Собирать информацию о всех приложениях для Server/Data Center версий продуктов Atlassian
- Получать историю версий приложений
- Загружать бинарные файлы (JAR/OBR)
- Просматривать собранные данные через веб-интерфейс

### Поддерживаемые продукты

- Jira Software / Service Management / Core
- Confluence Server / Data Center
- Bitbucket Server / Data Center
- Bamboo Server / Data Center
- Crowd Server / Data Center

## Установка

### Требования

- Python 3.8 или выше
- Интернет-соединение для доступа к Atlassian Marketplace API
- Учетные данные Atlassian (опционально, для некоторых функций)

### Шаг 1: Клонирование проекта

Если проект еще не скачан, скопируйте его в нужную директорию.

### Шаг 2: Создание виртуального окружения

**Windows:**
```powershell
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python -m venv venv
source venv/bin/activate
```

### Шаг 3: Установка зависимостей

```bash
pip install -r requirements.txt
```

### Шаг 3.1: Установка Playwright браузера (для скачивания описаний плагинов)

Для скачивания описаний плагинов с полным контентом (включая JavaScript) требуется Playwright:

```bash
# После установки зависимостей
playwright install chromium
```

**Примечание:** Playwright автоматически устанавливается при использовании скрипта `install.ps1` на Windows.

### Шаг 4: Создание файла конфигурации

Создайте файл `.env` в корневой директории проекта:

```env
# Учетные данные Atlassian Marketplace (обязательно для скрапинга)
# Учетные данные Marketplace API
# Можно настроить через веб-интерфейс (Management → Credentials)
# или в файле .credentials.json (рекомендуется)
# 
# Для одной учетной записи (старый формат, обратная совместимость):
MARKETPLACE_USERNAME=your-email@example.com
MARKETPLACE_API_TOKEN=your-api-token

# Для нескольких учетных записей используйте веб-интерфейс или создайте .credentials.json:
# {
#   "accounts": [
#     {"username": "user1@example.com", "api_token": "token1"},
#     {"username": "user2@example.com", "api_token": "token2"}
#   ]
# }

# Настройки скрапера (опционально)
SCRAPER_BATCH_SIZE=50
SCRAPER_REQUEST_DELAY=0.5
VERSION_AGE_LIMIT_DAYS=365
MAX_CONCURRENT_DOWNLOADS=3
MAX_VERSION_SCRAPER_WORKERS=10
MAX_RETRY_ATTEMPTS=3

# Настройки Flask (опционально)
FLASK_PORT=5000
FLASK_DEBUG=True
# SECRET_KEY генерируется автоматически при установке
# Для ручной генерации: python -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=your-secret-key-here

# Хранилище данных (опционально)
USE_SQLITE=False

# Пути для хранения данных (опционально)
# Можно указать отдельный диск для хранения больших объемов данных
# Windows пример: D:\marketplace-data
# Linux/Mac пример: /mnt/storage/marketplace-data
DATA_BASE_DIR=D:\marketplace-data
# Или указать пути отдельно:
# METADATA_DIR=D:\marketplace-data\metadata
# BINARIES_DIR=E:\marketplace-binaries
# LOGS_DIR=C:\marketplace-logs
```

**Как получить API токен:**
1. Войдите в свой аккаунт Atlassian
2. Перейдите в [Account Settings](https://id.atlassian.com/manage-profile/security/api-tokens)
3. Нажмите "Create API token"
4. Скопируйте токен в файл `.env`

## Настройка

### Базовые настройки

Минимально необходимые параметры в `.env`:
- `MARKETPLACE_USERNAME` — ваш email
- `MARKETPLACE_API_TOKEN` — токен API

### Дополнительные настройки

**SCRAPER_BATCH_SIZE** (по умолчанию: 50)
- Количество приложений, запрашиваемых за один API запрос
- Увеличение может ускорить сбор, но повышает риск rate limiting

**SCRAPER_REQUEST_DELAY** (по умолчанию: 0.5)
- Задержка между запросами в секундах
- Увеличение снижает нагрузку на API

**VERSION_AGE_LIMIT_DAYS** (по умолчанию: 365)
- Фильтр по возрасту версий (только версии за последние N дней)
- Установите 0 для сбора всех версий

**MAX_CONCURRENT_DOWNLOADS** (по умолчанию: 3)
- Количество параллельных загрузок
- Увеличение ускоряет загрузку, но требует больше ресурсов

**MAX_VERSION_SCRAPER_WORKERS** (по умолчанию: 10)
- Количество параллельных воркеров для сбора версий
- Увеличение ускоряет сбор версий

**USE_SQLITE** (по умолчанию: True)
- Использовать SQLite вместо JSON файлов
- Рекомендуется для больших объемов данных
- **Примечание:** SQLite требует установки, но обычно уже включен в Python

**Настройка путей для хранения данных**

Если у вас несколько дисков и вы хотите хранить данные на отдельном диске:

**Вариант 1: Указать базовую директорию для всех данных**
```env
# Windows пример (диск D:)
DATA_BASE_DIR=D:\marketplace-data

# Linux/Mac пример
DATA_BASE_DIR=/mnt/storage/marketplace-data
```
При этом структура будет:
- `D:\marketplace-data\data\metadata\` — метаданные
- `D:\marketplace-data\data\binaries\` — бинарные файлы

**Вариант 2: Указать пути отдельно (для разных дисков)**
```env
# Метаданные на диске C:
METADATA_DIR=C:\marketplace\metadata

# Бинарные файлы на диске D: (4TB)
BINARIES_DIR=D:\marketplace-binaries

# Логи на диске C:
LOGS_DIR=C:\marketplace-logs

# База данных SQLite (если используется)
DATABASE_PATH=C:\marketplace\marketplace.db
```

**Вариант 3: Разделить бинарники по дискам**
Если у вас несколько дисков по 4TB, можно вручную распределить загрузки, указав разные пути для разных продуктов (требует модификации кода) или использовать символические ссылки.

**Примеры для Windows:**
```env
# Все данные на диске D:
DATA_BASE_DIR=D:\marketplace-data

# Или отдельно:
BINARIES_DIR=E:\marketplace-binaries
METADATA_DIR=D:\marketplace-metadata
```

**Примеры для Linux/Mac:**
```env
# Все данные на внешнем диске
DATA_BASE_DIR=/mnt/storage/marketplace

# Или отдельно:
BINARIES_DIR=/mnt/disk1/binaries
METADATA_DIR=/mnt/disk2/metadata
```

**Распределение по продуктам:**

Для распределения бинарных файлов разных продуктов по разным дискам используйте:

```env
# Jira на диске H:
BINARIES_DIR_JIRA=H:\marketplace-binaries\jira

# Confluence на диске K:
BINARIES_DIR_CONFLUENCE=K:\marketplace-binaries\confluence

# Bitbucket на диске V:
BINARIES_DIR_BITBUCKET=V:\marketplace-binaries\bitbucket

# Bamboo на диске W:
BINARIES_DIR_BAMBOO=W:\marketplace-binaries\bamboo

# Crowd на диске F:
BINARIES_DIR_CROWD=F:\marketplace-binaries\crowd
```

**Важно:**
- Убедитесь, что указанные пути существуют или имеют права на создание
- Пути создаются автоматически при первом запуске
- Используйте абсолютные пути (полные пути от корня диска)
- Для Windows используйте обратные слэши `\` или прямые `/` (оба работают)
- Если путь для продукта не указан, используется `BINARIES_BASE_DIR`

## Использование

### Процесс работы состоит из 5 этапов:

1. **Сбор приложений** — получение списка всех приложений
2. **Сбор версий** — получение информации о версиях
3. **Загрузка бинарников** — скачивание JAR/OBR файлов
4. **Скачивание описаний** — скачивание описаний плагинов с картинками и видео
5. **Просмотр данных** — использование веб-интерфейса

---

### Этап 1: Сбор приложений

Запустите скрипт для сбора всех приложений:

```bash
python run_scraper.py
```

**Что происходит:**
- Скрипт проходит по всем продуктам (Jira, Confluence, Bitbucket, Bamboo, Crowd)
- Для каждого продукта собирает приложения, поддерживающие Server/Data Center
- Сохраняет метаданные в `data/metadata/apps.json` (или SQLite)
- Создает чекпоинты каждые 100 приложений

**Время выполнения:** ~5-10 минут для всех продуктов

**Возобновление после прерывания:**
```bash
python run_scraper.py --resume
```

**Результат:**
- Файл `data/metadata/apps.json` с метаданными всех приложений
- Или таблица `apps` в `data/metadata/marketplace.db` (если USE_SQLITE=True)

---

### Этап 2: Сбор версий

После сбора приложений запустите сбор версий:

```bash
python run_version_scraper.py
```

**Что происходит:**
- Для каждого приложения получает список версий
- Фильтрует версии:
  - Только за последние 365 дней (настраивается через `VERSION_AGE_LIMIT_DAYS`)
  - Только Server/Data Center (Cloud версии пропускаются)
- Сохраняет версии в `data/metadata/versions/{app_key}_versions.json`
- Использует параллельную обработку (10 воркеров по умолчанию)

**Время выполнения:** ~1-2 часа (зависит от количества приложений)

**Результат:**
- Файлы версий для каждого приложения в `data/metadata/versions/`
- Или таблица `versions` в SQLite базе

**Примечание:** Если приложение не имеет версий, соответствующий файл не создается.

---

### Этап 3: Загрузка бинарников

После сбора версий можно загрузить бинарные файлы:

```bash
# Загрузка всех версий для всех продуктов
python run_downloader.py

# Загрузка только для конкретного продукта
python run_downloader.py jira
python run_downloader.py confluence
```

**Что происходит:**
- Находит все версии, которые еще не загружены
- Загружает JAR/OBR файлы параллельно (3 потока по умолчанию)
- Сохраняет файлы в `data/binaries/{product}/{app_key}/{version_id}/`
- Обновляет статус загрузки в метаданных

**Время выполнения:** Зависит от количества версий и размера файлов (может занять часы)

**Особенности:**
- Пропускает уже загруженные файлы
- Автоматические повторы при ошибках (до 3 попыток)
- Проверка размера файла после загрузки

**Результат:**
- Бинарные файлы в `data/binaries/`
- Обновленные метаданные с информацией о загрузке

---

### Этап 4: Скачивание описаний плагинов

После загрузки бинарников можно скачать описания плагинов со страниц Marketplace:

```bash
# Скачивание описаний для всех приложений
python run_description_downloader.py

# Скачивание описаний для конкретного приложения
python run_description_downloader.py --addon-key com.example.app

# Скачивание с медиа-файлами (картинки, видео)
python run_description_downloader.py --download-media
```

**Что происходит:**
- Для каждого приложения открывается страница Marketplace в headless браузере (Playwright)
- Выполняется JavaScript для полной загрузки контента
- Сохраняется страница в формате HTML (с исправленными путями для офлайн-режима)
- Автоматически извлекается ссылка на документацию вендора из блока Resources
- Опционально скачиваются медиа-файлы (картинки, видео) отдельно
- Скачивание для всех типов хостинга (datacenter, server, cloud) при наличии URL

**Требования:**
- Playwright должен быть установлен: `playwright install chromium`
- Для работы требуется интернет-соединение

**Время выполнения:** Зависит от количества приложений (может занять несколько часов)

**Особенности:**
- Использует Playwright для выполнения JavaScript (SPA страницы)
- Сохраняет полностью отрендеренную страницу
- HTML файлы включают все ресурсы с исправленными путями
- Работает оффлайн после скачивания (вкладки и кнопки функциональны)
- **Автоматическое извлечение ссылки на документацию вендора**
- Сохранение `documentation_url` в JSON файле для отображения в веб-интерфейсе

**Результат:**
- HTML файлы в `{DESCRIPTIONS_DIR}/{addon_key}/full_page/index.html`
- JSON файлы с метаданными и ссылкой на документацию
- Медиа-файлы (если включено) в `{DESCRIPTIONS_DIR}/{addon_key}/media/`

**Примечание:** Если Playwright не установлен, используется fallback метод (удаление скриптов), который может не содержать полный контент, загружаемый через JavaScript.

---

### Этап 5: Просмотр данных

Запустите веб-интерфейс:

```bash
python app.py
```

Откройте браузер и перейдите по адресу: **http://localhost:5000**

**Возможности веб-интерфейса:**

1. **Главная страница (Dashboard)**
   - Статистика: количество приложений, версий, загруженных файлов
   - Использование дискового пространства
   - Ссылки на список приложений

2. **Список приложений (`/apps`)**
   - Фильтрация по продукту (Jira, Confluence и т.д.)
   - Поиск по названию, вендору или ключу
   - Пагинация (50 приложений на страницу)
   - Ссылки на детали каждого приложения

3. **Детали приложения (`/apps/<addon_key>`)**
   - Полная информация о приложении
   - Список всех версий (отсортированы по дате)
   - Ссылки на загрузку бинарников (если загружены)
   - Информация о совместимости

4. **API endpoints**
   - `/api/apps` — JSON список приложений
   - `/api/apps/<addon_key>` — JSON детали приложения
   - `/api/stats` — статистика
   - `/api/products` — список продуктов

---

## Веб-интерфейс

### Производительность

Веб-интерфейс оптимизирован для быстрой загрузки:
- **Параллельная загрузка данных** - все API запросы выполняются одновременно
- **Кэширование** - статистика кэшируется на 5 минут
- **Ленивая загрузка** - детальная статистика загружается по требованию
- **Оптимизированные API** - легковесные ответы для быстрой загрузки
- **Умное автообновление** - интервал 10 секунд для снижения нагрузки

**Ожидаемое время загрузки страниц:**
- Главная страница: < 200 мс
- Страница Management: < 500 мс
- Страница Storage: < 300 мс (с кэшем)

### Основные страницы

#### Главная страница (`/`)

Отображает:
- Общее количество приложений
- Общее количество версий
- Количество загруженных версий
- Использование дискового пространства
- Виджет статистики хранилища (с ленивой загрузкой)
- Ссылки на продукты

#### Список приложений (`/apps`)

**Фильтры:**
- **Продукт:** Выберите продукт из выпадающего списка
- **Поиск:** Введите название, вендора или ключ приложения

**Пагинация:**
- По умолчанию 50 приложений на страницу
- Используйте кнопки "Предыдущая" / "Следующая" для навигации

#### Статистика хранилища (`/storage`)

Отображает:
- Детальную статистику по категориям (binaries, descriptions, metadata)
- Разбивку по дискам
- Размер каждой папки (топ 100 на категорию)
- Процент использования каждой папки

**Особенности:**
- Данные кэшируются на 5 минут
- Быстрая загрузка благодаря кэшированию
- Возможность обновления данных вручную

#### Детали приложения (`/apps/<addon_key>`)

Отображает:
- Название и описание
- Вендор
- Поддерживаемые продукты
- Категории
- Список версий с:
  - Номером версии
  - Датой выпуска
  - Информацией о совместимости
  - Ссылкой на загрузку (если файл загружен)

#### Управление (`/manage`)

Отображает:
- Управление задачами (запуск скриптов)
- Управление учетными записями Marketplace API
- Просмотр логов
- Настройки путей хранения данных

**Особенности:**
- Автоматическое обновление статуса задач каждые 10 секунд
- Параллельная загрузка данных при открытии страницы
- Оптимизированное отображение большого количества задач
- Возможность очистки завершенных задач для улучшения производительности

### API Endpoints

#### GET /api/apps

Получить список приложений в формате JSON.

**Параметры:**
- `product` — фильтр по продукту (jira, confluence и т.д.)
- `search` — поисковый запрос

**Пример:**
```bash
curl "http://localhost:5000/api/apps?product=jira&search=automation"
```

#### GET /api/apps/<addon_key>

Получить детали приложения и его версии.

**Пример:**
```bash
curl "http://localhost:5000/api/apps/com.example.app"
```

#### GET /api/stats

Получить статистику.

**Пример:**
```bash
curl "http://localhost:5000/api/stats"
```

#### GET /api/products

Получить список продуктов.

**Пример:**
```bash
curl "http://localhost:5000/api/products"
```

## Решение проблем

### Проблема: "Marketplace credentials not configured"

**Решение:**
1. Убедитесь, что файл `.env` существует в корне проекта
2. Проверьте, что `MARKETPLACE_USERNAME` и `MARKETPLACE_API_TOKEN` установлены
3. Перезапустите скрипт

### Проблема: "No apps found"

**Возможные причины:**
1. Нет интернет-соединения
2. Неверные учетные данные
3. API недоступен

**Решение:**
1. Проверьте интернет-соединение
2. Проверьте учетные данные в `.env`
3. Проверьте логи в `logs/scraper.log`

### Проблема: "HTTP 429 Too Many Requests"

**Решение:**
1. Увеличьте `SCRAPER_REQUEST_DELAY` в `.env` (например, до 1.0)
2. Уменьшите `SCRAPER_BATCH_SIZE` (например, до 25)
3. Подождите несколько минут и повторите

### Проблема: Загрузки прерываются

**Решение:**
1. Проверьте доступное место на диске
2. Проверьте стабильность интернет-соединения
3. Уменьшите `MAX_CONCURRENT_DOWNLOADS` (например, до 1)
4. Запустите загрузку снова — уже загруженные файлы будут пропущены

### Проблема: Веб-интерфейс не запускается

**Возможные причины:**
1. Порт 5000 занят
2. Ошибка в коде

**Решение:**
1. Измените `FLASK_PORT` в `.env` на другой порт (например, 5001)
2. Проверьте логи в консоли
3. Убедитесь, что данные собраны (существует `data/metadata/apps.json` или `marketplace.db`)

### Проблема: Медленный сбор версий

**Решение:**
1. Увеличьте `MAX_VERSION_SCRAPER_WORKERS` (например, до 20)
2. Учтите, что это увеличит нагрузку на API и может привести к rate limiting

### Проблема: Недостаточно места на диске

**Решение:**
1. Проверьте размер директории `data/binaries/`
2. Удалите ненужные файлы вручную
3. Установите `VERSION_AGE_LIMIT_DAYS` на меньшее значение (например, 180) для сбора только недавних версий

## Часто задаваемые вопросы

### Q: Нужны ли учетные данные для работы?

**A:** Да, для сбора данных из Marketplace API требуются учетные данные Atlassian. Создайте API токен в настройках аккаунта.

### Q: Можно ли собирать данные без загрузки бинарников?

**A:** Да, этапы 1 и 2 (сбор приложений и версий) не требуют загрузки файлов. Загрузка бинарников — опциональный этап 3.

### Q: Сколько места на диске нужно?

**A:** Зависит от объема данных:
- Метаданные: ~10-50 MB
- Бинарные файлы: 10-100 GB (для всех версий за год)

### Q: Можно ли собирать данные только для одного продукта?

**A:** Да, можно модифицировать код или использовать фильтры в веб-интерфейсе. Но скрипты по умолчанию собирают данные для всех продуктов.

### Q: Как часто нужно обновлять данные?

**A:** Рекомендуется запускать сбор версий раз в неделю или месяц для получения новых версий. Полный сбор приложений нужен реже (раз в несколько месяцев).

### Q: Можно ли использовать SQLite вместо JSON?

**A:** Да, установите `USE_SQLITE=True` в `.env`. SQLite более эффективен для больших объемов данных и быстрых запросов.

### Q: Что делать, если скрапинг прервался?

**A:** Для сбора приложений используйте `python run_scraper.py --resume`. Для версий и загрузок просто запустите скрипты снова — они пропустят уже обработанные данные.

### Q: Можно ли экспортировать данные?

**A:** Данные хранятся в JSON или SQLite, их можно экспортировать вручную. API endpoints также позволяют получать данные в JSON формате.

### Q: Поддерживаются ли Cloud версии?

**A:** Нет, скрапер собирает только Server/Data Center версии приложений. Cloud версии фильтруются автоматически.

### Q: Как изменить период сбора версий?

**A:** Измените `VERSION_AGE_LIMIT_DAYS` в `.env`. Например, `VERSION_AGE_LIMIT_DAYS=180` для сбора версий за последние 6 месяцев. Установите `0` для сбора всех версий.

### Q: Страницы загружаются медленно, что делать?

**A:** Веб-интерфейс оптимизирован для быстрой загрузки:
- Данные загружаются параллельно при открытии страниц
- Используется кэширование для статистики (5 минут)
- Детальная статистика загружается по требованию (ленивая загрузка)
- Автообновление происходит каждые 10 секунд (не 5)

Если страницы все еще загружаются медленно:
1. Проверьте, не запущено ли много фоновых задач
2. Очистите завершенные задачи через кнопку "Clear Completed Tasks"
3. Проверьте размер лог-файлов (большие файлы могут замедлять чтение)
4. Убедитесь, что антивирус не блокирует доступ к файлам

## Дополнительные советы

1. **Первый запуск:** Начните с малого — соберите данные для одного продукта, чтобы проверить работу.

2. **Мониторинг:** Следите за логами в `logs/` для отслеживания прогресса и ошибок.

3. **Резервное копирование:** Регулярно создавайте резервные копии директории `data/`.

4. **Производительность:** Для больших объемов данных используйте SQLite (`USE_SQLITE=True`).

5. **Безопасность:** Не публикуйте файл `.env` с вашими учетными данными.

## Поддержка

При возникновении проблем:
1. Проверьте логи в `logs/`
2. Убедитесь, что все зависимости установлены
3. Проверьте настройки в `.env`
4. Обратитесь к разделу "Решение проблем" выше

---

**Удачного использования!**

